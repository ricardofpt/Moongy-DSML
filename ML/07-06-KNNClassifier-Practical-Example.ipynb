{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7490f66b",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classifier - Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ff38a",
   "metadata": {},
   "source": [
    "### Introducing the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c2d73",
   "metadata": {},
   "source": [
    "The database for this example is randomly generated. Generating datasets is an important skill to have. This is the best way to explore the algorithms themselves without having to worry about encoding, standardizing, dealing with outliers and all sorts of preprocessing that comes with a real-world dataset. Another advantage of randomly generated datasets is that you have full control over the data. You are free to choose the number of features, the number of classes, the way the points are distributed, everything really! \n",
    "\n",
    "I wish you to have lots of fun with the exercise and keep experimenting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc77e2",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function which will generate the random set of datapoints \n",
    "# and will distribute them into a specified number of classes\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# A module for handling data\n",
    "import pandas as pd\n",
    "\n",
    "# Python's plotting module. \n",
    "# We improve the graphics by overriding the default matplotlib styles with those of seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# A method used to split the dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The K-nearest neighbors classifier from the sklearn library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics that would allow us to evaluate our model\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "# A class that would help us find the best model from a specified set of models.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# A function that would plot for us the decision regions of a problem\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# This library will be used to measure the difference in times between the start and the end of a process\n",
    "import time\n",
    "\n",
    "# The Python package for scientific computing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238dfb7",
   "metadata": {},
   "source": [
    "### Generating the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed36080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normally distributed datapoints belonging to three separate clusters.\n",
    "# Let the total number of points be 1000.\n",
    "# Let us choose a center for each of the three classes.\n",
    "# The 'random state' parameter ensures that each run would produce the same distribution of points\n",
    "inputs, target = make_blobs(n_samples = 1000, \n",
    "                            centers = [(-3, 3), (0, 0), (2, 2)], \n",
    "                            random_state = 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea895cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the shapes of the inputs and the targets.\n",
    "# We see that 'inputs' consists of 1000 rows and 2 columns while\n",
    "# 'targets' stores 1000 items.\n",
    "inputs.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ddd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'inputs' and 'target' variables from above, let's create a dataframe.\n",
    "# The 'inputs' variable stores the two 'features' which represent the x- and y-coordinates of each datapoint.\n",
    "# The 'target' variable stores the classes.\n",
    "data = pd.DataFrame(data = inputs, columns = ['Feature 1', 'Feature 2'])\n",
    "data['Target'] = target\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0369b0b",
   "metadata": {},
   "source": [
    "### Plotting the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color palette\n",
    "knn_palette = sns.color_palette(['#000C1F', '#29757A', '#FF5050'])\n",
    "knn_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style\n",
    "sns.set()\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize = (16, 9))\n",
    "\n",
    "# Create a scatter plot.\n",
    "# On the x-axis, we have the first feature.\n",
    "# The y-axis we have placed the second feature.\n",
    "# The 'data' parameter specifies the dataset we are drawing the columns from.\n",
    "# The 'hue' parameter specifies the feature based on which the points are going to be colored.\n",
    "# The 'palette' parameter specifies the colors to be used in the plot.\n",
    "# The 'markers' parameter determines the shape of the points.\n",
    "# The 'style' parameter connects markers to classes.\n",
    "# The 's' parameter specifies the size of the points.\n",
    "# The 'alpha' parameter controls the opacity of the datapoints.\n",
    "# We have decided to set the 'legend' parameter equal to False as we will make no use of it in this example.\n",
    "sns.scatterplot(x = 'Feature 1', y = 'Feature 2', \n",
    "                data = data, \n",
    "                hue = 'Target', \n",
    "                palette = knn_palette,\n",
    "                markers = [',', '^', 'P'],\n",
    "                style = 'Target', \n",
    "                s = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e51ce",
   "metadata": {},
   "source": [
    "### Visualizing the distribution of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style\n",
    "sns.set()\n",
    "\n",
    "# Create a seaborn jointplot.\n",
    "sns.jointplot(x = 'Feature 1', y = 'Feature 2', \n",
    "              data = data, \n",
    "              hue = 'Target', \n",
    "              palette = knn_palette,\n",
    "              height = 10,\n",
    "              s = 100, \n",
    "              legend = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c4f53",
   "metadata": {},
   "source": [
    "### Creating a train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64226408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and a test dataset.\n",
    "# Choose the test size such that 20% of the data goes to testing.\n",
    "# Since 'train_test_split()' distributes the points randomly, we set a seed equal to 365\n",
    "# so that the final results are identical each time we run the split.\n",
    "# The 'stratify' argument allows for splitting the data in such a way that\n",
    "# the training and the testing datasets contain an equal portion of samples\n",
    "# from both classes.\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=365, \n",
    "                                                    stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb16f7",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier and set a specific number of neighbours\n",
    "clf = KNeighborsClassifier(n_neighbors = 2)\n",
    "\n",
    "# Fit the data\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c463f5a",
   "metadata": {},
   "source": [
    "### Predicting a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3040b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinates of a point whose class we want to predict.\n",
    "feature_1 = -0.18\n",
    "feature_2 = 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of a sample given these two features.\n",
    "clf.predict([[feature_1, feature_2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe43c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distance to the neighbour and its index in the array of the training dataset\n",
    "neighbors = clf.kneighbors([[feature_1, feature_2]])\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97bc45a",
   "metadata": {},
   "source": [
    "### Visualizing the neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "\n",
    "# Plot the datapoints from the training data\n",
    "sns.scatterplot(x = x_train[:, 0], y = x_train[:, 1],\n",
    "                hue = y_train, \n",
    "                palette = knn_palette,\n",
    "                markers = [',', '^', 'P'],\n",
    "                style = y_train, \n",
    "                s = 100, \n",
    "                legend = True);\n",
    "\n",
    "# Plot the point to be predicted\n",
    "sns.scatterplot(x = [feature_1], y = [feature_2], \n",
    "                style = [feature_2],\n",
    "                markers = ['o'],\n",
    "                s = 100,\n",
    "                legend = False);\n",
    "\n",
    "# A list to store the x-values of all neighbors\n",
    "plot_x_train = []\n",
    "# A list to store the y-values of all neighbors\n",
    "plot_y_train = []\n",
    "\n",
    "# Append all x- and y-values to the respective lists\n",
    "for i in neighbors[1]:\n",
    "    plot_x_train.append(x_train[i, 0])\n",
    "    plot_y_train.append(x_train[i, 1])\n",
    "\n",
    "# Enclose all nearest neighbors\n",
    "plt.scatter(plot_x_train, \n",
    "            plot_y_train, \n",
    "            s=200, facecolors='none', edgecolors='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a49b14",
   "metadata": {},
   "source": [
    "### Getting the parameters of the model\n",
    "\n",
    "Something strange has happened, right? The predicted class is 0 (the squares) but by looking at the plot above there's a closer point belonging to class 2 (the crosses).\n",
    "\n",
    "So why did the class 0 has been chosen? Take a look at the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters of the classifier\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b19b9a-cf3d-4682-a328-64d01a56a448",
   "metadata": {},
   "source": [
    "Notice the weights parameter: it's set to _uniform_, not to _distance_. When set to uniform and a tie happens, the class with most tied data points wins; here we have only one data point per class, so the next win factor goes to the index of the class, winning the smallest one, in this case 0.\n",
    "\n",
    "Let's run the classifier again, specifying the distance as the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c175b-777d-4b5c-bb90-21afb16a961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier and set a specific number of neighbours\n",
    "clf = KNeighborsClassifier(n_neighbors = 2, weights = 'distance')\n",
    "\n",
    "# Fit the data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Define the coordinates of a point whose class we want to predict.\n",
    "feature_1 = -0.18\n",
    "feature_2 = 3.2\n",
    "\n",
    "# Predict the class of a sample given these two features.\n",
    "clf.predict([[feature_1, feature_2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a1d17-3274-4113-abb0-19de3acf602e",
   "metadata": {},
   "source": [
    "Now we got class 2, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd14bee",
   "metadata": {},
   "source": [
    "### Drawing the decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580546aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the shape of the x_train array\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the time at which the operation has begun\n",
    "start = time.time()\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "\n",
    "# Plot the decision regions.\n",
    "# The 'X' and 'y' parameters are the x- and y-coordinates of the training data.\n",
    "# 'X_highlight' represents a datapoint which we want to highlight - in this case,\n",
    "# the sample whose class we want to predict.\n",
    "# Choose the classifier with the 'clf' parameter\n",
    "# With 'markers' and 'colors' we choose the shape and color of each datapoint.\n",
    "# With 'scatter_kwargs' we list additional parameters in the form of a dictionary.\n",
    "# In our case, these are size of the datapoints, the color of their edges, as well as\n",
    "# the transparacy.\n",
    "# legend = 0 hides the legend.\n",
    "plot_decision_regions(X = x_train, y = y_train,\n",
    "                      X_highlight = np.array([[feature_1, feature_2]]),\n",
    "                      clf = clf,\n",
    "                      markers = [',', '^', 'P'],\n",
    "                      colors = '#000c1f,#29757a,#ff5050',\n",
    "                      scatter_kwargs = {'s':100, 'edgecolor':'white', 'alpha':1},\n",
    "                      legend = 0)\n",
    "\n",
    "# Give labels to the axes\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2');\n",
    "\n",
    "# Store the time at which the operation has ended\n",
    "end = time.time()\n",
    "\n",
    "# Print the difference between the two times\n",
    "print(f'Time elapsed: {round(end - start, 1)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aa225",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[feature_1, feature_2]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd318194",
   "metadata": {},
   "source": [
    "### Test what's the best k value (n_neighbors) to choose option 1: Getting the error rates of a set of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists which will store the error rates for models with\n",
    "# uniform or distance weights.\n",
    "error_uniform = []\n",
    "error_distance = []\n",
    "\n",
    "# The range of K-values we will cover\n",
    "k_range = range(1, 51)\n",
    "\n",
    "# For each K value, create a classifier with both type of weights.\n",
    "# In both cases, fit the model to the trining data.\n",
    "# Make predictions on the test data.\n",
    "# Append the error rate as 1-accuracy.\n",
    "for k in k_range:\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors = k, weights = 'uniform')\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    error_uniform.append(1 - accuracy_score(y_test, predictions))\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors = k, weights = 'distance')\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    error_distance.append(1 - accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8ecd",
   "metadata": {},
   "source": [
    "#### Plotting the error rates as a function of the number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 9))\n",
    "\n",
    "# Plot the error rates.\n",
    "# The models with uniform weights are plotted with a solid blue line.\n",
    "# The models with distance weights are plotted with a dashed green line.\n",
    "# The circular markers mark the distinct K-values.\n",
    "plt.plot(k_range, error_uniform, c = 'blue', linestyle = 'solid', \n",
    "         marker = 'o', markerfacecolor = 'red',  label = 'Error uniform');\n",
    "plt.plot(k_range, error_distance, c = 'green', linestyle = 'dashed', \n",
    "         marker = 'o', markerfacecolor = 'red', label = 'Error distance');\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('K-value')\n",
    "plt.ylabel('Error rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511ebe6",
   "metadata": {},
   "source": [
    "### Test what's the best k value (n_neighbors) to choose option 2: Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a set of parameters to test in the form of a dictionary.\n",
    "parameters = {'n_neighbors':range(1, 51), \n",
    "              'weights':['uniform', 'distance']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5fc29",
   "metadata": {},
   "source": [
    "#### Create an instance of the GridSearchCV class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea05d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GridSearchCV class.\n",
    "# We choose KNeighborsClassifier as an estimator.\n",
    "# Choose the dictionary of parameters we want to cover.\n",
    "# Choose a scoring metric. I have decided to go with accuracy.\n",
    "grid_search = GridSearchCV(estimator = KNeighborsClassifier(), \n",
    "                           param_grid = parameters, \n",
    "                           scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all models to the training data and choose the best one based on the highest accuracy.\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the parameters of the best model\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ecb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 'clf' an instance of the best model.\n",
    "clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the score obtained by the best model.\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c8440",
   "metadata": {},
   "source": [
    "### Make predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b010455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the classes of the samples in the test dataset using the best model.\n",
    "y_test_pred = clf.predict(x_test)\n",
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834cc42",
   "metadata": {},
   "source": [
    "### Construct the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1978bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the seaborn visualization removes the white lines that come with it.\n",
    "sns.reset_orig()\n",
    "\n",
    "# Create a confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_test_pred,\n",
    "    labels = clf.classes_,\n",
    "    cmap = 'magma'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fb5cb",
   "metadata": {},
   "source": [
    "### Print out the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9432038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classifcation report\n",
    "print(classification_report(y_test, y_test_pred, target_names = ['0', '1', '2']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
