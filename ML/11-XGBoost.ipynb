{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  XGBoost\n",
    "\n",
    "## Instalation\n",
    "\n",
    "PIP: pip install --user xgboost\n",
    "\n",
    "CPU only: conda install -c conda-forge py-xgboost-cpu\n",
    "\n",
    "Use NVIDIA GPU: conda install -c conda-forge py-xgboost-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introducing XGBoost\n",
    "\n",
    "Boosting, especially of decision trees, is among the most prevalent and powerful machine learning algorithms.\n",
    "\n",
    "There are many variants of boosting algorithms and frameworks implementing those algorithms. XGBoost — short for extreme gradient boosting — is one of the most well-known algorithms with an accompanying, and even more popular, framework.\n",
    "\n",
    "As the name may reveal, XGBoost is a gradient boosting algorithm, a common technique in ensemble learning. Ensemble learning is a type of machine learning that enlists many models to make predictions together. \n",
    "\n",
    "Boosting algorithms are distinguished from other ensemble learning techniques by building a sequence of initially weak models into increasingly more powerful models. Gradient boosting algorithms choose how to build a more powerful model using the gradient of a loss function that captures the performance of a model.\n",
    "\n",
    "XGBoost operates on decision trees, models that construct a graph that examines the input under various “if” statements (vertices in the graph). Whether the “if” condition is satisfied influences the next “if” condition and eventual prediction. XGBoost progressively adds more and more “if” conditions to the decision tree to build a stronger model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example - XGBoost regression\n",
    "\n",
    "### Loading and exploring the data\n",
    "\n",
    "We will be working with the Diamonds dataset throughout the tutorial. It is built into the Seaborn library. It has a nice combination of numeric and categorical features and over 50k observations that we can comfortably showcase all the advantages of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just look at the 5-number summary of the numeric and categorical features and get going. You can spend a few moments to familiarize yourself with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53940, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940</td>\n",
       "      <td>53940</td>\n",
       "      <td>53940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>21551</td>\n",
       "      <td>11292</td>\n",
       "      <td>13065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cut  color clarity\n",
       "count   53940  53940   53940\n",
       "unique      5      7       8\n",
       "top     Ideal      G     SI1\n",
       "freq    21551  11292   13065"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.describe(exclude=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an XGBoost DMatrix\n",
    "\n",
    "After you are done with exploration, the first step in any project is framing the machine learning problem and extracting the feature and target arrays based on the dataset.\n",
    "\n",
    "We will first try to predict diamond prices using their physical measurements, so our target will be the `price` column.\n",
    "\n",
    "So, we are isolating the features into `X` and the target into `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract feature and target arrays\n",
    "X, y = diamonds.drop('price', axis=1), diamonds[['price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has three categorical columns. Normally, you would encode them with ordinal or one-hot encoding, but XGBoost has the ability to internally deal with categoricals.\n",
    "\n",
    "The way to enable this feature is to cast the categorical columns into Pandas category data type (by default, they are treated as text columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat       float64\n",
       "cut        category\n",
       "color      category\n",
       "clarity    category\n",
       "depth       float64\n",
       "table       float64\n",
       "x           float64\n",
       "y           float64\n",
       "z           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to Pandas category\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')\n",
    "\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s split the data into train, and test sets (0.25 test size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the important part: XGBoost comes with its own class for storing datasets called DMatrix. It is a highly optimized class for memory and speed. That's why converting datasets into this format is a requirement for the native XGBoost API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class accepts both the training features and the labels. To enable automatic encoding of Pandas category columns, we also set enable_categorical to True.\n",
    "\n",
    "After building the DMatrices, you should choose a value for the `objective` parameter. It tells XGBoost the machine learning problem you are trying to solve and what metrics or loss functions to use to solve that problem.\n",
    "\n",
    "For example, to predict diamond prices, which is a regression problem, you can use the common `reg:squarederror` objective. Usually, the name of the objective also contains the name of the loss function for the problem. For regression, it is common to use Root Mean Squared Error.\n",
    "\n",
    "> A note on the difference between a loss function and a performance metric: A loss function is used by machine learning models to minimize the *differences* between the actual (ground truth) values and model predictions. On the other hand, a metric (or metrics) is chosen by the machine learning engineer to measure the *similarity* between ground truth and model predictions.\n",
    "\n",
    "### Training\n",
    "\n",
    "The chosen objective function and any other hyperparameters of XGBoost should be specified in a dictionary, which by convention should be called params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set another parameter called `num_boost_round`, which stands for number of boosting rounds. Internally, XGBoost minimizes the loss function RMSE in small incremental rounds (more on this later). This parameter specifies the amount of those rounds.\n",
    "\n",
    "The ideal number of rounds is found through hyperparameter tuning. For now, we will just set it to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "\n",
    "n = 100\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg, #our train matrix\n",
    "   num_boost_round=n,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "During the boosting rounds, the model object has learned all the patterns of the training set it possibly can. Now, we must measure its performance by testing it on unseen data. That's where our `dtest_reg` DMatrix comes into play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the base model: 555.607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = model.predict(dtest_reg)\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "print(f\"RMSE of the base model: {round(rmse, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve got a base score ~555$, which was the performance of a base model with default parameters. There are two ways we can improve it — by performing cross-validation and hyperparameter tuning. But before that, let’s see a quicker way of evaluating XGBoost models.\n",
    "\n",
    "### Using Validation Sets During Training\n",
    "\n",
    "Training a machine learning model is like launching a rocket into space. You can control everything about the model up to the launch, but once it does, all you can do is stand by and wait for it to finish.\n",
    "\n",
    "But the problem with our current training process is that we can’t even watch where the model is going. To solve this, we will use evaluation arrays that allow us to see model performance as it gets improved incrementally across boosting rounds.\n",
    "\n",
    "Let’s create a list of two tuples that each contain two elements. The first element is the array for the model to evaluate, and the second is the array’s name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass this array to the `evals` parameter of `xgb.train`, we will see the model performance after each boosting round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.29379\tvalidation-rmse:2817.38773\n",
      "[1]\ttrain-rmse:2092.07711\tvalidation-rmse:2054.73630\n",
      "[2]\ttrain-rmse:1549.52687\tvalidation-rmse:1526.30592\n",
      "[3]\ttrain-rmse:1184.46798\tvalidation-rmse:1174.90119\n",
      "[4]\ttrain-rmse:941.09127\tvalidation-rmse:943.28272\n",
      "[5]\ttrain-rmse:784.58014\tvalidation-rmse:796.09651\n",
      "[6]\ttrain-rmse:685.75110\tvalidation-rmse:705.22245\n",
      "[7]\ttrain-rmse:624.67281\tvalidation-rmse:653.32563\n",
      "[8]\ttrain-rmse:584.19599\tvalidation-rmse:620.30404\n",
      "[9]\ttrain-rmse:558.77667\tvalidation-rmse:599.24504\n",
      "[10]\ttrain-rmse:543.85303\tvalidation-rmse:586.99790\n",
      "[11]\ttrain-rmse:531.92694\tvalidation-rmse:578.68120\n",
      "[12]\ttrain-rmse:523.08456\tvalidation-rmse:571.73527\n",
      "[13]\ttrain-rmse:515.67753\tvalidation-rmse:567.19913\n",
      "[14]\ttrain-rmse:510.77594\tvalidation-rmse:564.66402\n",
      "[15]\ttrain-rmse:506.68519\tvalidation-rmse:563.21547\n",
      "[16]\ttrain-rmse:502.96796\tvalidation-rmse:561.80880\n",
      "[17]\ttrain-rmse:498.90184\tvalidation-rmse:560.36561\n",
      "[18]\ttrain-rmse:492.74859\tvalidation-rmse:558.46274\n",
      "[19]\ttrain-rmse:490.30278\tvalidation-rmse:556.87216\n",
      "[20]\ttrain-rmse:487.42071\tvalidation-rmse:556.44229\n",
      "[21]\ttrain-rmse:484.74496\tvalidation-rmse:556.55429\n",
      "[22]\ttrain-rmse:480.95735\tvalidation-rmse:557.84139\n",
      "[23]\ttrain-rmse:478.48520\tvalidation-rmse:557.89540\n",
      "[24]\ttrain-rmse:475.23956\tvalidation-rmse:557.37962\n",
      "[25]\ttrain-rmse:471.61791\tvalidation-rmse:556.87508\n",
      "[26]\ttrain-rmse:469.65231\tvalidation-rmse:556.70128\n",
      "[27]\ttrain-rmse:466.45165\tvalidation-rmse:555.73740\n",
      "[28]\ttrain-rmse:464.66200\tvalidation-rmse:555.11206\n",
      "[29]\ttrain-rmse:463.36324\tvalidation-rmse:555.09142\n",
      "[30]\ttrain-rmse:460.86396\tvalidation-rmse:554.68339\n",
      "[31]\ttrain-rmse:459.68274\tvalidation-rmse:554.79977\n",
      "[32]\ttrain-rmse:457.48581\tvalidation-rmse:554.57599\n",
      "[33]\ttrain-rmse:455.07939\tvalidation-rmse:555.65575\n",
      "[34]\ttrain-rmse:454.03028\tvalidation-rmse:555.26394\n",
      "[35]\ttrain-rmse:452.35989\tvalidation-rmse:554.62246\n",
      "[36]\ttrain-rmse:449.38579\tvalidation-rmse:553.35840\n",
      "[37]\ttrain-rmse:448.64842\tvalidation-rmse:553.45248\n",
      "[38]\ttrain-rmse:448.47633\tvalidation-rmse:553.35233\n",
      "[39]\ttrain-rmse:445.58979\tvalidation-rmse:553.05158\n",
      "[40]\ttrain-rmse:444.03762\tvalidation-rmse:552.62130\n",
      "[41]\ttrain-rmse:442.01167\tvalidation-rmse:552.97558\n",
      "[42]\ttrain-rmse:441.35357\tvalidation-rmse:553.23244\n",
      "[43]\ttrain-rmse:440.74163\tvalidation-rmse:553.14153\n",
      "[44]\ttrain-rmse:440.60815\tvalidation-rmse:553.05782\n",
      "[45]\ttrain-rmse:439.48758\tvalidation-rmse:553.30981\n",
      "[46]\ttrain-rmse:438.70697\tvalidation-rmse:553.22313\n",
      "[47]\ttrain-rmse:435.38239\tvalidation-rmse:553.74845\n",
      "[48]\ttrain-rmse:434.17988\tvalidation-rmse:553.24786\n",
      "[49]\ttrain-rmse:432.53983\tvalidation-rmse:553.43480\n",
      "[50]\ttrain-rmse:430.07110\tvalidation-rmse:553.50718\n",
      "[51]\ttrain-rmse:429.02843\tvalidation-rmse:553.68181\n",
      "[52]\ttrain-rmse:428.82789\tvalidation-rmse:553.55179\n",
      "[53]\ttrain-rmse:426.65097\tvalidation-rmse:554.33720\n",
      "[54]\ttrain-rmse:425.35817\tvalidation-rmse:555.00412\n",
      "[55]\ttrain-rmse:424.43950\tvalidation-rmse:555.04530\n",
      "[56]\ttrain-rmse:423.98886\tvalidation-rmse:555.23862\n",
      "[57]\ttrain-rmse:423.37385\tvalidation-rmse:555.01524\n",
      "[58]\ttrain-rmse:422.89152\tvalidation-rmse:554.73559\n",
      "[59]\ttrain-rmse:420.17877\tvalidation-rmse:555.28689\n",
      "[60]\ttrain-rmse:418.57995\tvalidation-rmse:555.44368\n",
      "[61]\ttrain-rmse:416.15098\tvalidation-rmse:556.22395\n",
      "[62]\ttrain-rmse:415.20122\tvalidation-rmse:556.03493\n",
      "[63]\ttrain-rmse:413.94128\tvalidation-rmse:555.89869\n",
      "[64]\ttrain-rmse:412.14760\tvalidation-rmse:555.56320\n",
      "[65]\ttrain-rmse:412.07284\tvalidation-rmse:555.51809\n",
      "[66]\ttrain-rmse:411.12288\tvalidation-rmse:556.53856\n",
      "[67]\ttrain-rmse:409.86233\tvalidation-rmse:556.02134\n",
      "[68]\ttrain-rmse:408.98845\tvalidation-rmse:555.64995\n",
      "[69]\ttrain-rmse:407.60369\tvalidation-rmse:555.40507\n",
      "[70]\ttrain-rmse:406.77489\tvalidation-rmse:555.06703\n",
      "[71]\ttrain-rmse:405.55576\tvalidation-rmse:554.85987\n",
      "[72]\ttrain-rmse:404.01644\tvalidation-rmse:554.78435\n",
      "[73]\ttrain-rmse:402.82464\tvalidation-rmse:554.70160\n",
      "[74]\ttrain-rmse:401.31075\tvalidation-rmse:554.40804\n",
      "[75]\ttrain-rmse:399.88578\tvalidation-rmse:554.56453\n",
      "[76]\ttrain-rmse:399.46555\tvalidation-rmse:554.83544\n",
      "[77]\ttrain-rmse:398.97065\tvalidation-rmse:554.83387\n",
      "[78]\ttrain-rmse:398.50360\tvalidation-rmse:554.66181\n",
      "[79]\ttrain-rmse:396.24144\tvalidation-rmse:555.31061\n",
      "[80]\ttrain-rmse:394.18070\tvalidation-rmse:555.00800\n",
      "[81]\ttrain-rmse:392.71336\tvalidation-rmse:555.25449\n",
      "[82]\ttrain-rmse:392.14361\tvalidation-rmse:555.18259\n",
      "[83]\ttrain-rmse:390.73064\tvalidation-rmse:555.24386\n",
      "[84]\ttrain-rmse:390.32572\tvalidation-rmse:555.02768\n",
      "[85]\ttrain-rmse:388.65251\tvalidation-rmse:555.33841\n",
      "[86]\ttrain-rmse:387.44200\tvalidation-rmse:555.24404\n",
      "[87]\ttrain-rmse:386.96057\tvalidation-rmse:555.09808\n",
      "[88]\ttrain-rmse:385.78554\tvalidation-rmse:554.95239\n",
      "[89]\ttrain-rmse:384.05170\tvalidation-rmse:555.95636\n",
      "[90]\ttrain-rmse:382.65353\tvalidation-rmse:555.74725\n",
      "[91]\ttrain-rmse:381.97534\tvalidation-rmse:555.69323\n",
      "[92]\ttrain-rmse:380.47646\tvalidation-rmse:555.26134\n",
      "[93]\ttrain-rmse:379.11827\tvalidation-rmse:554.93934\n",
      "[94]\ttrain-rmse:377.82598\tvalidation-rmse:555.07593\n",
      "[95]\ttrain-rmse:376.66849\tvalidation-rmse:555.53397\n",
      "[96]\ttrain-rmse:374.97521\tvalidation-rmse:555.60914\n",
      "[97]\ttrain-rmse:374.61227\tvalidation-rmse:555.61898\n",
      "[98]\ttrain-rmse:373.84881\tvalidation-rmse:555.65889\n",
      "[99]\ttrain-rmse:373.74308\tvalidation-rmse:555.60692\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the model minimizes the score after each round.\n",
    "\n",
    "In real-world projects, you usually train for thousands of boosting rounds, which means that many rows of output. To reduce them, you can use the `verbose_eval` parameter, which forces XGBoost to print performance updates every `vebose_eval` rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.29379\tvalidation-rmse:2817.38773\n",
      "[10]\ttrain-rmse:543.85303\tvalidation-rmse:586.99790\n",
      "[20]\ttrain-rmse:487.42071\tvalidation-rmse:556.44229\n",
      "[30]\ttrain-rmse:460.86396\tvalidation-rmse:554.68339\n",
      "[40]\ttrain-rmse:444.03762\tvalidation-rmse:552.62130\n",
      "[50]\ttrain-rmse:430.07110\tvalidation-rmse:553.50718\n",
      "[60]\ttrain-rmse:418.57995\tvalidation-rmse:555.44368\n",
      "[70]\ttrain-rmse:406.77489\tvalidation-rmse:555.06703\n",
      "[80]\ttrain-rmse:394.18070\tvalidation-rmse:555.00800\n",
      "[90]\ttrain-rmse:382.65353\tvalidation-rmse:555.74725\n",
      "[99]\ttrain-rmse:373.74308\tvalidation-rmse:555.60692\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=10 # Every ten rounds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Early Stopping\n",
    "\n",
    "By now, you must have realized how important boosting rounds are. Generally, the more rounds there are, the more XGBoost tries to minimize the loss. But this doesn’t mean the loss will always go down. Let’s try with 3500 boosting rounds with the verbosity of 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.29379\tvalidation-rmse:2817.38773\n",
      "[500]\ttrain-rmse:197.72375\tvalidation-rmse:563.29248\n",
      "[1000]\ttrain-rmse:121.69016\tvalidation-rmse:572.18689\n",
      "[1500]\ttrain-rmse:84.17960\tvalidation-rmse:576.42698\n",
      "[2000]\ttrain-rmse:60.88001\tvalidation-rmse:578.58142\n",
      "[2500]\ttrain-rmse:46.47975\tvalidation-rmse:579.96944\n",
      "[3000]\ttrain-rmse:36.59124\tvalidation-rmse:580.78514\n",
      "[3499]\ttrain-rmse:29.49081\tvalidation-rmse:581.36713\n"
     ]
    }
   ],
   "source": [
    "n = 3500\n",
    "\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the lowest loss before round 500. After that, even though training loss keeps going down, the validation loss (the one we care about) keeps increasing.\n",
    "\n",
    "When given an unnecessary number of boosting rounds, XGBoost starts to overfit and memorize the dataset. This, in turn, leads to validation performance drop because the model is memorizing instead of generalizing.\n",
    "\n",
    "Remember, we want the golden middle: a model that learned just enough patterns in training that it gives the highest performance on the validation set. So, how do we find the perfect number of boosting rounds, then?\n",
    "\n",
    "We will use a technique called early stopping. Early stopping forces XGBoost to watch the validation loss, and if it stops improving for a specified number of rounds, it automatically stops training.\n",
    "\n",
    "This means we can set as high a number of boosting rounds as long as we set a sensible number of early stopping rounds.\n",
    "\n",
    "For example, let’s use 10000 boosting rounds and set the `early_stopping_rounds` parameter to 50. This way, XGBoost will automatically stop the training if validation loss doesn't improve for 50 consecutive rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.29379\tvalidation-rmse:2817.38773\n",
      "[50]\ttrain-rmse:430.07110\tvalidation-rmse:553.50718\n",
      "[89]\ttrain-rmse:384.05170\tvalidation-rmse:555.95636\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=50,\n",
    "   # Activate early stopping\n",
    "   early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Cross-Validation\n",
    "\n",
    "At the beginning we set aside 25% of the dataset for testing. The test set would allow us to simulate the conditions of a model in production, where it must generate predictions for unseen data.\n",
    "\n",
    "But only a single test set would not be enough to measure how a model would perform in production accurately. For example, if we perform hyperparameter tuning using only a single training and a single test set, knowledge about the test set would still “leak out.” How?\n",
    "\n",
    "Since we try to find the best value of a hyperparameter by comparing the validation performance of the model on the test set, we will end up with a model that is configured to perform well only on that particular test set. Instead, we want a model that performs well across the board — on any test set we throw at it.\n",
    "\n",
    "The solution is cross-validation. In cross-validation, we still have two sets: training and testing.\n",
    "\n",
    "While the test set waits in the corner, we split the training into 3, 5, 7, or `k` splits or folds. Then, we train the model `k` times. Each time, we use `k-1` parts for training and the final `k`th part for validation. This process is called k-fold cross-validation.\n",
    "\n",
    "After all folds are done, we can take the mean of the scores as the final, most realistic performance of the model.\n",
    "\n",
    "Let’s perform this process in code using the `cv` function of XGB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   early_stopping_rounds=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2874.530912</td>\n",
       "      <td>9.576510</td>\n",
       "      <td>2877.437274</td>\n",
       "      <td>37.093540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2089.327469</td>\n",
       "      <td>8.317290</td>\n",
       "      <td>2094.021636</td>\n",
       "      <td>24.828795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1550.617973</td>\n",
       "      <td>5.223297</td>\n",
       "      <td>1558.386252</td>\n",
       "      <td>18.540267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1183.812759</td>\n",
       "      <td>5.193420</td>\n",
       "      <td>1195.032441</td>\n",
       "      <td>13.471580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>941.203113</td>\n",
       "      <td>4.539805</td>\n",
       "      <td>958.728828</td>\n",
       "      <td>9.479449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>784.505805</td>\n",
       "      <td>5.707896</td>\n",
       "      <td>807.806455</td>\n",
       "      <td>9.228214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>684.295140</td>\n",
       "      <td>4.217592</td>\n",
       "      <td>711.731819</td>\n",
       "      <td>9.488339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>620.605084</td>\n",
       "      <td>3.991168</td>\n",
       "      <td>652.864677</td>\n",
       "      <td>10.612015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>581.944546</td>\n",
       "      <td>4.668284</td>\n",
       "      <td>617.502530</td>\n",
       "      <td>10.319193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>556.851294</td>\n",
       "      <td>4.102871</td>\n",
       "      <td>596.822911</td>\n",
       "      <td>11.646375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0      2874.530912        9.576510     2877.437274      37.093540\n",
       "1      2089.327469        8.317290     2094.021636      24.828795\n",
       "2      1550.617973        5.223297     1558.386252      18.540267\n",
       "3      1183.812759        5.193420     1195.032441      13.471580\n",
       "4       941.203113        4.539805      958.728828       9.479449\n",
       "5       784.505805        5.707896      807.806455       9.228214\n",
       "6       684.295140        4.217592      711.731819       9.488339\n",
       "7       620.605084        3.991168      652.864677      10.612015\n",
       "8       581.944546        4.668284      617.502530      10.319193\n",
       "9       556.851294        4.102871      596.822911      11.646375"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549.311480649509"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse = results['test-rmse-mean'].min()\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example 2 - XGBoost Classification\n",
    "\n",
    "Building an XGBoost classifier is as easy as changing the objective function; the rest can stay the same.\n",
    "\n",
    "The two most popular classification objectives are:\n",
    "\n",
    "* `binary:logistic` - binary classification (the target contains only two classes, i.e., cat or dog)\n",
    "* `multi:softprob` - multi-class classification (more than two classes in the target, i.e., apple/orange/banana)\n",
    "\n",
    "Performing binary and multi-class classification in XGBoost is almost identical, so we will go with the latter. Let’s prepare the data for the task first.\n",
    "\n",
    "We want to predict the cut quality of diamonds given their price and their physical measurements. So, we will build the feature/target arrays accordingly.\n",
    "\n",
    "The only difference is that since XGBoost only accepts numbers in the target, we are encoding the text classes in the target with `OrdinalEncoder` of Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "X, y = diamonds.drop(\"cut\", axis=1), diamonds[['cut']]\n",
    "\n",
    "# Encode y to numeric\n",
    "y_encoded = OrdinalEncoder().fit_transform(y)\n",
    "\n",
    "# Extract text features\n",
    "cats = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Convert to pd.Categorical\n",
    "for col in cats:\n",
    "   X[col] = X[col].astype('category')\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build the DMatrices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…and set the objective to `multi:softprob`. This objective also requires the number of classes to be set by us (`cut` has 5 unique values, so the `num_class` param will be 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multi:softprob\", \"tree_method\": \"hist\", \"num_class\": 5}\n",
    "n = 100\n",
    "\n",
    "results = xgb.cv(\n",
    "   params, dtrain_clf,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   metrics=[\"mlogloss\", \"auc\", \"merror\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross-validation, we are asking XGBoost to watch three classification metrics which report model performance from three different angles. Here is the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross-validation, we are asking XGBoost to watch three classification metrics which report model performance from three different angles. Here is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['train-mlogloss-mean', 'train-mlogloss-std', 'train-auc-mean',\n",
       "       'train-auc-std', 'train-merror-mean', 'train-merror-std',\n",
       "       'test-mlogloss-mean', 'test-mlogloss-std', 'test-auc-mean',\n",
       "       'test-auc-std', 'test-merror-mean', 'test-merror-std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to see the best AUC score, we take the maximum of test-auc-mean column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387768185864125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['test-auc-mean'].max()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "machine.learning",
   "language": "python",
   "name": "machine.learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
