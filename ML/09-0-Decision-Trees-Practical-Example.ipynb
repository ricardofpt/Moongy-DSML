{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees - Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load the iris dataset straight from sklearn\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check what the input looks like\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the target looks like\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that there are 150 samples, each with 4 features\n",
    "# The 4 features are: Sepal length, Sepal width, Petal length and Petal width\n",
    "np.shape(X)\n",
    "\n",
    "# For the target (y), consider 0 = Setosa, 1 = Versicolour and 2 = Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to define the decision tree and its parameters, if any\n",
    "# Then, we need to train/create the tree based on the data\n",
    "# Both are easily achieved through sklearn, with 2 simple commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tree classifier\n",
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/creating the decision tree\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we have created a fully working decision tree for the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now give this tree an input, and it will predict the class of the flower (Versicolour because the output is 1)\n",
    "clf.predict([[6.1, 2.7, 3.9, 1.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sklearn, we also have capabilities to plot the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default plot_tree function returns a list with text information about the tree and also plots a small image of it\n",
    "plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better visualize it, we can use matplotlib, and control the size of the figure\n",
    "plt.figure(figsize=(15,12))\n",
    "plot_tree(clf, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As can be seen from the image above, though, the features and classes don't have names\n",
    "# It is not informative, as we don't know what feature X[2] corresponds to\n",
    "# So, we can add a list of feature and class names to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the feature and class names, as well\n",
    "plt.figure(figsize=(15,12))\n",
    "plot_tree(clf, filled=True, \n",
    "          feature_names=[\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"], \n",
    "          class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"])\n",
    "plt.show()\n",
    "\n",
    "# See if you can interpret the tree. Remember the predicting values were [6.1, 2.7, 3.9, 1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret\n",
    "\n",
    "* Nodes are the questions, with 4 pieces of information:\n",
    "    1. the question itself, ex: Petal Width <= 0.8\n",
    "    2. gini impurity - measures the misclassification of the data points. Zero means that there's no misclassification, so it's value should be as closest to zero as possible.\n",
    "    3. the number of data points available\n",
    "    4. the number of datapoints per class or target\n",
    "* Branches are the answers:\n",
    "    1. left branch is True\n",
    "    2. right branch is False\n",
    "* Leaves are the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: Playing with _pruning_\n",
    "\n",
    "Add the pruning hyperparameter (_ccp_alpha_) to the classifier and try different values to see the different outcomes of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Exercise Solution: Playing with _pruning_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tree classifier\n",
    "clf = DecisionTreeClassifier(ccp_alpha=0.1)\n",
    "# Training/creating the decision tree\n",
    "clf = clf.fit(X, y)\n",
    "# We can now give this tree an input, and it will predict the class of the flower (Versicolour because the output is 1)\n",
    "clf.predict([[6.1, 2.7, 3.9, 1.2]])\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(15,12))\n",
    "plot_tree(clf, filled=True, \n",
    "          feature_names=[\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"], \n",
    "          class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
