{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9936bff7-46ff-4ee3-8533-cf5b474ce9d3",
   "metadata": {},
   "source": [
    "# Phase 1 - Extraction\n",
    "\n",
    "Let's remember our data source's locations:\n",
    "\n",
    "* `films.db` - project folder\n",
    "* `customer_list.xlsx` - https://drive.google.com/drive/folders/1DefyOI3Qn7nthICx5t55DIBrc_PblsIQ?usp=sharing\n",
    "* `stores.html` and `staff.html` - http://clevernet.pt/python_course/stores.html and http://clevernet.pt/python_course/staff.html\n",
    "* `payments.json` - project folder\n",
    "* `inventory-store-1.csv` and `inventory-store-2.csv` (zipped) - https://drive.google.com/drive/folders/1DefyOI3Qn7nthICx5t55DIBrc_PblsIQ?usp=sharing\n",
    "* `rentals` files - FTP account with the following credentials: \n",
    "    * server: ftp.oom.pt\n",
    "    * username: pyproj@oom.pt\n",
    "    * password: J8rk#{4)L$~6YNnOZ%\n",
    "    \n",
    "So we'll have to find a way to deal with the following:\n",
    "\n",
    "* connect to a database and work with data using SQL (in our case, SQLite)\n",
    "* connect to Google Drive and retrieve files\n",
    "* open and work with excel files\n",
    "* scrape a website and extract information from it\n",
    "* open and work with json files\n",
    "* deal with zipped files\n",
    "* open and work with CSV files\n",
    "* connect to an FTP account and retrieve files\n",
    "* open and work with TXT files\n",
    "\n",
    "For now we'll just focus on gathering the data, not working with it.\n",
    "\n",
    "Let's start coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86120b-6952-4317-8444-0d0dfcaf95d9",
   "metadata": {},
   "source": [
    "## Extracting from sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6cedd4-1a1f-4be5-8341-3b3e90aba590",
   "metadata": {},
   "source": [
    "### SQL databases\n",
    "\n",
    "There's multiple ways of interacting with a database in python: either by a specific connector package (like the python's *sqlite3* package) or using a package like *pandas* that allows you to query the database ad get data from it.\n",
    "\n",
    "However, we'll go a different route, and we'll use a package called *sqlalchemy*. This package is called an ORM (Object Relational Mapper) and it provides a single syntax that you can use in a lot of different databases, such as SQLite, MySQL, MS SQL Server or Oracle DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d68df8-5703-44cf-9542-2935740db864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the create_engine() function will create a new db if it doesn't exist and open it if it does\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///data_sources/films.db') \n",
    "\n",
    "engine.connect()\n",
    " \n",
    "print(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade22a9-5d66-44f9-a049-5c9a0c21aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "\n",
    "# the MetaData object holds all the information about the database and the tables it contains. \n",
    "# we use an instance of it to create or drop tables in the database.\n",
    "metadata = MetaData()\n",
    "\n",
    "# we'll populate the instance with our db\n",
    "metadata.reflect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971b547-e7b1-4533-9a26-285533bedd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tables property to list our tables\n",
    "for table in metadata.tables:\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b69817-0b8f-4453-a006-b3b74c27421a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can query the table details as well:\n",
    "for table in metadata.tables:\n",
    "    print(f'Information for table: {table}')\n",
    "    table_obj = metadata.tables[table]\n",
    "    print('-' * 40)\n",
    "    for col in table_obj.columns:\n",
    "        print(f\"{col.name} - {col.type}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b55836-653c-416f-8792-5f7ee3f999f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.sql import text\n",
    "\n",
    "# we can select data using connect():\n",
    "with engine.connect() as conn:\n",
    "    query = conn.execute(text('SELECT count(*) as row_count from film'))\n",
    "    result = query.fetchone()\n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde37a9-bb8e-4e6f-a4f3-4397b64f85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or using a session:\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# initial configuration arguments\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# this session is bound to provided engine\n",
    "session = Session()\n",
    "\n",
    "query = session.query(metadata.tables['film'])\n",
    "print(query.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605956c-58b5-4a4e-a82e-b4c0bc80d3a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can update the output above with more information:\n",
    "for table in metadata.tables:\n",
    "    table_obj = metadata.tables[table]\n",
    "    query = session.query(table_obj)\n",
    "    print(f'Information for table: {table} - {query.count()} rows')\n",
    "    print('-' * 40)\n",
    "    for col in table_obj.columns:\n",
    "        print(f\"{col.name} - {col.type}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab8557-07cd-4194-b7d7-f56c8b436100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we can actually select data. To select 10 films from the films table that were released in 2006, we could do this:\n",
    "table_obj = metadata.tables['film']\n",
    "query = session.query(table_obj).filter(table_obj.c.release_year == '2006').limit(10)\n",
    "for row in query:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44360501-4f31-4e0d-b102-b1450a565954",
   "metadata": {},
   "source": [
    "### Google Drive\n",
    "\n",
    "To access google drive you have to have an account at google cloud, and then create an app to access the drive API. That's beyond the scope of this course and everything has been taken care of (the result is the `client_secrets.json` file in the *config* folder).\n",
    "\n",
    "You'll probably need to install the pydrive package:\n",
    "\n",
    "* if using anaconda: conda install -c conda-forge pydrive\n",
    "* else: pip install pydrive\n",
    "\n",
    "Before executing the cell below, here's some information of what is going to happen:\n",
    "\n",
    "1. A new tab will open asking for credentials. Use these ones:\n",
    "    * email: pthnprjct@gmail.com\n",
    "    * password: Tdm1ZWfpvSXg44H85h\n",
    "2. Press *continue* if you get an alert about the app being in test mode\n",
    "3. Allow access if asked for permissions\n",
    "4. Everything is ok when you get the message *The authentication flow has completed.*\n",
    "5. You can close the tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51f53c-16a4-4f44-96a3-ad606dcb3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to google drive\n",
    "from pydrive.auth import GoogleAuth\n",
    "\n",
    "GoogleAuth.DEFAULT_SETTINGS['client_config_file'] = 'configs/client_secrets.json'\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050a97f-29ef-4621-ab36-f733731c804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the first time authenticating, you can save your credentials\n",
    "gauth.SaveCredentialsFile(\"configs/drive_credentials.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeedbc28-b2c2-4032-a139-1958db80bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so actually you can change the connection and authentication process to this (with the option to refresh an expired access token):\n",
    "from pydrive.auth import GoogleAuth\n",
    "\n",
    "GoogleAuth.DEFAULT_SETTINGS['client_config_file'] = 'configs/client_secrets.json'\n",
    "gauth = GoogleAuth()\n",
    "# try to load saved client credentials\n",
    "gauth.LoadCredentialsFile(\"configs/drive_credentials.txt\")\n",
    "\n",
    "if gauth.credentials is None:\n",
    "    # authenticate if they're not there\n",
    "    gauth.LocalWebserverAuth()\n",
    "elif gauth.access_token_expired:\n",
    "    # refresh them if expired\n",
    "    gauth.Refresh()\n",
    "else:\n",
    "    # initialize the saved credentials\n",
    "    gauth.Authorize()\n",
    "\n",
    "# save the current credentials to a file\n",
    "gauth.SaveCredentialsFile(\"configs/drive_credentials.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b22c44-abe7-495b-9af7-924e32cb71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local instance of your drive\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# show files (excluding folders and deleted files) in the drive\n",
    "file_list = drive.ListFile({'q': 'mimeType != \"application/vnd.google-apps.folder\" and trashed=false'}).GetList()\n",
    "\n",
    "for file in file_list:\n",
    "    print(file['title'], file['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137b907-c30a-46ae-ada1-43ff27ba4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download files\n",
    "for file in file_list:\n",
    "    if file['title'] == 'inventories.zip' or file['title'] == 'customer_list.xlsx':\n",
    "        file = drive.CreateFile({'id': file['id']})\n",
    "        file.GetContentFile(f\"data_sources/{file['title']}\")\n",
    "        print(f\"Successfully downloaded {file['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a50e83b-1c22-468e-a663-a7bbbce39db3",
   "metadata": {},
   "source": [
    "### Excel files\n",
    "\n",
    "The easiest way of reading excel files is using the *pandas* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457d92-43fd-49dd-8f37-356998a59a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_xlsx = pd.read_excel('data_sources/customer_list.xlsx')\n",
    "\n",
    "df_xlsx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744aed6-3959-41e5-b7cd-b5920e14d09a",
   "metadata": {},
   "source": [
    "### Zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4a8d9-ea0e-42ac-b64f-c42114d2fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('data_sources/inventories.zip', 'r') as zipObj:\n",
    "   # extract all the contents of zip file in different directory\n",
    "   zipObj.extractall('data_sources')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961fc01-888f-43d5-8faf-0d063e465e16",
   "metadata": {},
   "source": [
    "### Web scraping\n",
    "\n",
    "To do scraping a minimal understanding of HTML is needed, since we'll be selecting portions of the page through the manipulation of HTML tags in the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0b293-6574-4c6c-ac0c-655969593280",
   "metadata": {},
   "source": [
    "#### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4ddb6-52fd-47a2-a2f1-78f058152db6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's request the page and save the response:\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import requests\n",
    "\n",
    "request = requests.get('http://clevernet.pt/python_course/stores.html').text\n",
    "response = bsoup(request, 'html5lib')\n",
    "    \n",
    "print(response.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac9e3e-03ff-4527-a30b-fc167396b5e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the table from the page\n",
    "table = response.find('table')\n",
    "print(table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6355b-2200-45aa-98f4-ed09b509e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the rows inside the table\n",
    "rows = table.find_all('tr')\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987835a-7c20-4417-b5a7-71d6e56a9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll save the information into a dict\n",
    "data = {}\n",
    "\n",
    "# iterate the rows to get the data we want\n",
    "for i, row in enumerate(rows):\n",
    "    # we don't need the first row\n",
    "    if i != 0:\n",
    "        # if it's the second row, it's the headers, else it's content (we'll treat them differently)\n",
    "        if i == 1:\n",
    "            # get all the cells inside the row\n",
    "            cells = row.find_all('th')\n",
    "            # iterate the cells\n",
    "            for cell in cells:\n",
    "                data[cell.get_text()] = []\n",
    "        else:\n",
    "            # get all the cells inside the row\n",
    "            cells = row.find_all('td')\n",
    "            # iterate the cells\n",
    "            for j, cell in enumerate(cells):\n",
    "                dict_keys = list(data.keys())\n",
    "                data[dict_keys[j]].append(cell.get_text())\n",
    "print(data)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9370f-f757-4c0c-9068-ef9bc515fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can easily send this to a pandas dataframe\n",
    "df_stores = pd.DataFrame.from_dict(data)\n",
    "\n",
    "df_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113d65d-95d1-468b-ae2e-03c2835b10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's export the dataframe to pickle, because we'll need it later on\n",
    "# pickle is a great way of saving your dataframe without having to convert it to another type\n",
    "df_stores.to_pickle('data_sources/stores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee58bda-d943-42dd-8fd7-7d9b5d3b8631",
   "metadata": {},
   "source": [
    "#### Staff\n",
    "\n",
    "This will be almost the same as the stores, so the code will be more compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc5c52-909f-4c28-9c97-8e1df53c1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import requests\n",
    "\n",
    "request = requests.get('http://clevernet.pt/python_course/staff.html').text\n",
    "response = bsoup(request, 'html5lib')\n",
    "\n",
    "table = response.find('table')\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "data = {}\n",
    "for i, row in enumerate(rows):\n",
    "    if i != 0:\n",
    "        if i == 1:\n",
    "            cells = row.find_all('th')\n",
    "            for cell in cells:\n",
    "                data[cell.get_text()] = []\n",
    "        else:\n",
    "            cells = row.find_all('td')\n",
    "            for j, cell in enumerate(cells):\n",
    "                dict_keys = list(data.keys())\n",
    "                data[dict_keys[j]].append(cell.get_text())\n",
    "\n",
    "df_staff = pd.DataFrame.from_dict(data)\n",
    "\n",
    "df_staff.to_pickle('data_sources/staff.pkl')\n",
    "\n",
    "df_staff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109397df-0275-45d6-8ffd-428f6c7ad459",
   "metadata": {},
   "source": [
    "### JSON files\n",
    "\n",
    "The easiest way of reading json files is using the *pandas* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec326e39-ace2-446e-8e12-af3a5bb822d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json('data_sources/payments.json')\n",
    "\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69a193-afcf-4156-96fd-df4d8a1036b3",
   "metadata": {},
   "source": [
    "That didn't work as expected! This happens quite often because a json file might have multiple formats.\n",
    "\n",
    "If you look at the raw json file you'll notice that all the records (each record is a dict) are the values of the key `Payments`. Furthermore, they are all inside a list.\n",
    "\n",
    "So we have to do two things: \n",
    "* first, load the json file into a python dict using the *json* package (since it's a dict we can now access the part we're interested in)\n",
    "* second, use `json_normalize` instead of `read_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c1c23-6380-46b9-a6bd-17eb1dcb7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data_sources/payments.json','r') as file:\n",
    "    data = json.loads(file.read())\n",
    "    \n",
    "# notice that we're calling normalize not on the whole dict, but only on the values of the 'Payments' key\n",
    "df_json = pd.json_normalize(data['Payments'])\n",
    "\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7dc326-0029-4e91-b768-4333d4433117",
   "metadata": {},
   "source": [
    "### CSV files\n",
    "\n",
    "The easiest way of reading csv files is using the *pandas* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385b356-ea21-4b0f-8c90-72e2eb7f6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('data_sources/inventory-store-1.csv', delimiter=';')\n",
    "\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc4e92-db2a-40f2-bfa8-078c48213fd1",
   "metadata": {},
   "source": [
    "Or you can use the *csv* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c86e19-5955-43ea-88cc-b3bb50a5ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data_sources/inventory-store-1.csv') as file:\n",
    "    csv_reader = csv.reader(file, delimiter=';')\n",
    "    for i, line in enumerate(csv_reader):\n",
    "        if i < 6:\n",
    "            print(line)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03133812-3fa2-457f-b37b-f928620aff71",
   "metadata": {},
   "source": [
    "### FTP\n",
    "\n",
    "FTP stands for File Transfer Protocol. It's as if we were accessing a hard drive in another computer, where we can browse the files and folders and upload/download what we want.\n",
    "\n",
    "Python has built-in support for FTP through the *ftplib* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64231d98-17e3-468c-a25a-2850a83215a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP, all_errors\n",
    "# this is a local import. We're importing the file ftp_credencials.py from the configs directory.\n",
    "from configs import ftp_credentials\n",
    "\n",
    "# connect to ftp server\n",
    "try:\n",
    "    ftp = FTP(ftp_credentials.config['server'], ftp_credentials.config['user'], ftp_credentials.config['pass'])\n",
    "    print('Connected successfully.')\n",
    "except KeyError:\n",
    "    print('Missing credentials.')\n",
    "except all_errors as e:\n",
    "    print(f'Error while trying to connect -> {e}')\n",
    "except Exception as e:\n",
    "    print(f'Error -> {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec889ee1-1710-4561-94cc-3e61856bd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files/folders in server\n",
    "listing = ftp.nlst()\n",
    "print(listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915f3be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7252622-3ee1-4a50-bc9e-5e018ce8bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to enter the rentals folder and download all the files into our own rentals folder, inside the data_sources folder\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# if folder is empty\n",
    "if len(listing) == 0:\n",
    "    print('The folder is empty.')\n",
    "# switch to the data_sources folder    \n",
    "os.chdir('data_sources')   \n",
    "\n",
    "# create or empty the rentals folder\n",
    "if not os.path.exists('rentals'):\n",
    "    os.mkdir('rentals')\n",
    "else:\n",
    "    shutil.rmtree('rentals') # this deletes the folder and all the files inside recursively\n",
    "    os.mkdir('rentals')\n",
    "\n",
    "# swith to the rentals folder in your disk\n",
    "os.chdir('rentals')\n",
    "# swith to the rentals folder in the server\n",
    "ftp.cwd('/rentals')\n",
    "# list files in server folder\n",
    "file_listing = ftp.nlst()\n",
    "print(file_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36805b-35fe-4463-b693-3f270cb8c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle each file\n",
    "for filename in file_listing:\n",
    "    if(filename.endswith('txt')):\n",
    "        file = os.path.join(os.getcwd(), filename)\n",
    "        try:\n",
    "            with open(file, 'wb') as local_file:\n",
    "                ftp.retrbinary('RETR ' + filename, local_file.write)\n",
    "            print(f'Successfully downloaded {filename}.')   \n",
    "        except all_errors as e:\n",
    "            print(f'Error while trying to download -> {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3f8c7-e41a-4121-940b-ad3afaad705e",
   "metadata": {},
   "source": [
    "### Off-topic: knowing which folder is your current working directory (cwd) is very important\n",
    "\n",
    "In the cells above we've changed our cwd using the `chdir()` method to facilitate our work. But we cannot forget that such a change will impact all the cells below, if we need to deal with paths.\n",
    "\n",
    "So it's a good practice to always change back to our base folder after using `chdir()`.\n",
    "\n",
    "Also bear in mind that if you run the cell below more than once, you'll keep going back in your path. If you wish to start over you have to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb98ec-9a6a-4706-86e7-4281e3b5499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch back to the main project folder, because the cells below are expecting you to be there\n",
    "# this is where I am right now (the path before the 'course_project' folder will be different from yours obviously)\n",
    "print(f'Before changing: {os.getcwd()}')\n",
    "# To go back to the 'course_project' folder:\n",
    "os.chdir('../..')\n",
    "print(f'After changing: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8162d8-3b5d-4599-b8e3-3ded6cda3629",
   "metadata": {},
   "source": [
    "### TXT files\n",
    "\n",
    "You can read text files the exact same way as you read csv, using the *pandas* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09727fc0-587b-4c11-9184-d7ecb1fd40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt = pd.read_csv('data_sources/rentals/rentals.txt', delimiter='|')\n",
    "\n",
    "df_txt.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872b9e9-81e6-42aa-a815-de08ea2641c9",
   "metadata": {},
   "source": [
    "Not exactly the best outcome since we don't need the first line, but we'll focus on that in the next phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4c350-5ff8-4680-b11c-744df820ada1",
   "metadata": {},
   "source": [
    "**Ok, so every data source is extracted. What can we still do to make our lives easier moving forward?**\n",
    "\n",
    "Open *1.2-extraction-consolidation.ipynb* and let's move forward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
