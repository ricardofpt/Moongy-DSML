{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bdb5657-89f9-4ad5-b367-20cc3c72b82e",
   "metadata": {},
   "source": [
    "# Data Consolidation\n",
    "\n",
    "Following up on the question from the previous notebook, here's what we decided to do:\n",
    "\n",
    "* join all the text files and delete the uneeded row\n",
    "* join the csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fdccf-081a-4d53-b945-db54be7ada30",
   "metadata": {},
   "source": [
    "## Joining files\n",
    "\n",
    "When joining files it's important to verify if the headers appear in every file or not. In our case they don't in the txt files, but they do in the csv files.\n",
    "\n",
    "We'll use two different ways: standard python for the txt files and *pandas* for the csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cbff5-b2bf-4726-b7ba-55b80a283e4d",
   "metadata": {},
   "source": [
    "### TXT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253b908-f584-4085-85d7-cb2685f5862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, chdir, getcwd\n",
    "from os.path import isfile, join\n",
    "\n",
    "# we're creating a list of all the files in the rentals folder\n",
    "chdir('data_sources/rentals')\n",
    "txt_files = [join(getcwd(), file) for file in listdir() if isfile(file)]\n",
    "chdir('../..')\n",
    "\n",
    "txt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b34be-f5cb-4cd2-8ae9-ed31f23497e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see the files aren't sorted, and we need to parse the rentals.txt first because it's the one that contains the headers\n",
    "txt_files = sorted(txt_files)\n",
    "txt_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcc93e-1eac-452b-98a9-596d35461552",
   "metadata": {},
   "source": [
    "It's obviously not a perfect sort, but it's enough for what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52a7f9-e154-4c2c-8bae-21bb0df0a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll use that list to join all the files together\n",
    "with open('data_sources/rentals_all.txt', 'w') as outfile:\n",
    "    for file in txt_files:\n",
    "        with open(file) as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "# if the files are big and you can't place them all in memory, you'll have to read them line by line             \n",
    "# with open('output_file', 'w') as outfile:\n",
    "#    for file in filenames:\n",
    "#        with open(file) as infile:\n",
    "#            for line in infile:\n",
    "#                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd413ec-3ddf-487c-9a30-8656b5b59648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if it went well\n",
    "import pandas as pd\n",
    "\n",
    "df_txt = pd.read_csv('data_sources/rentals/rentals.txt', delimiter='|')\n",
    "print(f\"Row count of just one txt file: {df_txt.shape[0]}\")\n",
    "\n",
    "df_rentals = pd.read_csv('data_sources/rentals_all.txt', delimiter='|')\n",
    "print(f\"Row count of all the txt files we parsed now: {df_rentals.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594bae0-a824-4af7-b1a1-ed5ad0531d1c",
   "metadata": {},
   "source": [
    "So if each file has 1000 rows, to get 16045 rows we would need 17 files. Sounds about right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2040d15-1473-487f-b8c2-6c305ac23eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64211b-cc58-497e-9b67-e79228bc2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can delete that first row\n",
    "df_rentals.drop(index=df_rentals.index[0], axis=0, inplace=True)\n",
    "\n",
    "df_rentals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c09d5-b5c5-4f4f-ba66-b97ada75a07f",
   "metadata": {},
   "source": [
    "### CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ec299-af34-4195-ba12-053047aa1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both csv's into dataframes\n",
    "df_csv_1 = pd.read_csv('data_sources/inventory-store-1.csv', delimiter=';')\n",
    "df_csv_2 = pd.read_csv('data_sources/inventory-store-2.csv', delimiter=';')\n",
    "\n",
    "# add them to a list and call the concat() method\n",
    "frames = [df_csv_1, df_csv_2]\n",
    "df_csv_final = pd.concat(frames)\n",
    "\n",
    "# verify if it's all good. Notice we're using the python function len(), an alternative to the shape property in pandas.\n",
    "print(f\"Row count of csv 1 + csv 2: {len(df_csv_1) + len(df_csv_2)}\")\n",
    "print(f\"Row count of csv's combined: {df_csv_final.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177a8c0-bdd2-427a-a943-fad617bdd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to a csv file\n",
    "df_csv_final.to_csv('data_sources/inventory-store-all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de9f31-a261-4a38-b15a-bc3f62da213c",
   "metadata": {},
   "source": [
    "## Deciding on an approach to build the data lake\n",
    "\n",
    "Since we're dealing with a small amount of data and we can access it from python, we could end the process right here and assume that our data lake is ready for the next phase. But since the next phase comprises building a unified data model, we'll have to do a lot of joins. \n",
    "\n",
    "We could certainly use *pandas* for that, but with a little bit of SQL knowledge we can get the task done in a much more easier way. Furthermore, we already have a database in our datasources, so why don't we capitalize on that and use it to load the other datasources into it?\n",
    "\n",
    "Since it's an SQLite database, the fastest and easier way to load records is by using csv files. We already have the following ready to go:\n",
    "\n",
    "* the *films.db*, our database\n",
    "* *inventories-stores-all.csv*\n",
    "\n",
    "So here's what we have to do next:\n",
    "\n",
    "* convert the *customer_list.xlsx* to csv\n",
    "* convert the *payments.json* to csv\n",
    "* convert the *rentals_all.txt* to csv\n",
    "* export the *staff* and *stores* data to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bddaf-5f8b-49c8-a768-f152dd03878b",
   "metadata": {},
   "source": [
    "## Converting/Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35117e3e-679e-4747-b457-ef5f74038f7d",
   "metadata": {},
   "source": [
    "### Excel, JSON and TXT files\n",
    "\n",
    "Notice that we'll be using `index=False` when converting to csv since we don't need the index column that was automatically generated by *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd05457-e382-476c-878a-32e8ffa31258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel - we're simply reusing the code from the previous notebook to create the dataframe\n",
    "df_xlsx = pd.read_excel('data_sources/customer_list.xlsx')\n",
    "df_xlsx.to_csv('data_sources/customer_list.csv', index=False)\n",
    "\n",
    "# json - we're simply reusing the code from the previous notebook to create the dataframe\n",
    "import json\n",
    "\n",
    "with open('data_sources/payments.json','r') as file:\n",
    "    data = json.loads(file.read())\n",
    "    \n",
    "df_json = pd.json_normalize(data['Payments'])\n",
    "df_json.to_csv('data_sources/payments.csv', index=False)\n",
    "\n",
    "# txt - we're using the dataframe created some cells above\n",
    "df_rentals.to_csv('data_sources/rentals_all.csv', index=False)\n",
    "\n",
    "# check the data_sources folder for the created files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40fa42-8026-4a4a-aebe-634481d9bf43",
   "metadata": {},
   "source": [
    "### Web scraped data\n",
    "\n",
    "We'll use the pickle files created earlier to export the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314548b-0eed-4336-a5c7-4017ac397f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores = pd.read_pickle('data_sources/stores.pkl')\n",
    "df_stores.to_csv('data_sources/stores.csv', index=False)\n",
    "\n",
    "df_staff = pd.read_pickle('data_sources/staff.pkl')\n",
    "df_staff.to_csv('data_sources/staff.csv', index=False)\n",
    "\n",
    "# check the data_sources folder for the created files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81866ccc-c82f-4729-9ac2-510a8ad410d0",
   "metadata": {},
   "source": [
    "## Building the data lake\n",
    "\n",
    "It's time to send all these csv's to the *films.db* to finally start inspecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934a9b5-02b5-4063-9b02-b4459012d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///data_sources/films.db') \n",
    "\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46796a39-154c-454c-afe9-41d3e7cc694c",
   "metadata": {},
   "source": [
    "There are several ways to do this, and we'll use two of them. The first is using *pandas*, it's suitable when the csv is small, and will parse half of our csv's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de566d6-1d0a-40b6-9305-a62ea51815d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv's to parse:\n",
    "to_parse = {\n",
    "    'store': 'stores.csv',\n",
    "    'staff': 'staff.csv',\n",
    "    'rental': 'rentals_all.csv',\n",
    "    'payment': 'payments.csv',\n",
    "    'inventory': 'inventory-store-all.csv',\n",
    "    'customer': 'customer_list.csv'\n",
    "}\n",
    "\n",
    "for key, value in to_parse.items():\n",
    "    # we're replacing any existing records in the table with the records from our csv\n",
    "    pd.read_csv(f'data_sources/{value}').to_sql(key, engine, if_exists='replace', index=False)\n",
    "    print(f'Successfully added {value} to database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7244ae-f932-4a18-86f5-4387c92c4450",
   "metadata": {},
   "source": [
    "The next one is directly using the SQLite app in your operating system, totally bypassing python, therefore much more performant and suitable for large files. We'll just use python to call the app, and that's it.\n",
    "\n",
    "> __Note__: this might not work in Windows unless you use a unix-like shell such as Cygwin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e3c4a-fa10-4538-9c3e-85682b5db257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# csv's to parse:\n",
    "to_parse = {\n",
    "    'payment': 'payments.csv',\n",
    "    'inventory': 'inventory-store-all.csv',\n",
    "    'customer': 'customer_list.csv'\n",
    "}\n",
    "\n",
    "for key, value in to_parse.items():\n",
    "    # drop the table if it already exists\n",
    "    #drop_cmd = f\"\"\"sqlite3 {os.path.join(os.getcwd(), 'data_sources', 'films.db')} <<< \"drop table if exists {key}\" \"\"\"\n",
    "    #print(drop_cmd)\n",
    "    #os.system(drop_cmd)\n",
    "    \n",
    "    #import the csv\n",
    "    import_cmd = f\"(echo .separator ,; echo .import {os.path.join(os.getcwd(), 'data_sources', value)} {key})\"\n",
    "    import_cmd += f\" | sqlite3 {os.path.join(os.getcwd(), 'data_sources', 'films.db')}\"\n",
    "    #print(import_cmd)\n",
    "    os.system(import_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a95cc-da84-4aa4-888c-23f4243c0824",
   "metadata": {},
   "source": [
    "## Quick data lake inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9d58f-b791-4d2e-9b45-15b36538b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "metadata = MetaData()\n",
    "metadata.reflect(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "for table in metadata.tables:\n",
    "    table_obj = metadata.tables[table]\n",
    "    query = session.query(table_obj)\n",
    "    print(f'Information for table: {table} - {query.count()} rows')\n",
    "    print('-' * 40)\n",
    "    for col in table_obj.columns:\n",
    "        print(f\"{col.name} - {col.type}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40a925-6371-42b1-9630-4e9c9f9c73b8",
   "metadata": {},
   "source": [
    "**Notice any difference between the tables imported with *pandas* and the other ones?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
