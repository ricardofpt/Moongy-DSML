{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with dataframes in python using Pandas\n",
    "\n",
    "Pandas is built on top of the **NumPy** package, meaning a lot of the structure of NumPy is used or replicated in Pandas. Data in pandas is often used to feed statistical analysis in **SciPy**, plotting functions from **Matplotlib**, and machine learning algorithms in **Scikit-learn**.\n",
    "\n",
    "> Note: this lesson will just scratch the surface of what is possible with Pandas. It's focused on most common commands and operations and should be seen as a starting point for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is usually imported like this:\n",
    "import pandas as pd\n",
    "# we can override default options of pandas, such as automatic column and row hiding when the dataset is to large, with the following commands:\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core components of pandas\n",
    "\n",
    "The primary two components of pandas are the `Series` and `DataFrame`.\n",
    "\n",
    "A `Series` is essentially a column, and a `DataFrame` is a multi-dimensional table made up of a collection of `Series`.\n",
    "\n",
    "## Creating DataFrames from scratch\n",
    "\n",
    "There are many ways to create a `DataFrame` from scratch, but a great option is to just use a simple `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'apples': [3, 2, 0, 1], \n",
    "    'oranges': [0, 3, 7, 2]\n",
    "}\n",
    "\n",
    "purchases = pd.DataFrame(data)\n",
    "\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Index` of this `DataFrame` was given to us on creation as the numbers 0-3, but we could also create our own when we initialize the `DataFrame`.\n",
    "\n",
    "Let's have customer names as our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data, index=['June', 'Robert', 'Lily', 'David'])\n",
    "\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we could locate a customer's order by using their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.loc['June']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How to read in data\n",
    "\n",
    "It’s quite simple to load data from various file formats into a `DataFrame`. In the following examples we'll keep using our apples and oranges data, but this time it's coming from various files.\n",
    "\n",
    "### Reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchases.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSVs don't have indexes like our `DataFrames`, so all we need to do is just designate the `index_col` when reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're setting the index to be column zero (the customers names)\n",
    "df = pd.read_csv('purchases.csv', index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('purchases.json')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from database\n",
    "\n",
    "If you’re working with data from a SQL database you need to first establish a connection using an appropriate Python library, then pass a query to pandas. Here we'll use SQLite to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"database.db\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM purchases\", con)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with CSVs, we could pass `index_col='index'`, but we can also set an index after-the-fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('index')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('purchases.xlsx', index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting back to a CSV, JSON, Database or Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_purchases.csv')\n",
    "\n",
    "df.to_json('new_purchases.json')\n",
    "\n",
    "df.to_sql('new_purchases', con, if_exists='replace')\n",
    "\n",
    "df.to_excel('new_purchases.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important DataFrame operations\n",
    "\n",
    "`DataFrames` possess hundreds of methods and other operations that are crucial to any analysis. As a beginner, you should know the operations that perform simple transformations of your data and those that provide fundamental statistical analysis.\n",
    "\n",
    "Let's load in the IMDB movies dataset to begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"IMDB-Movie-Data.csv\", index_col=\"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing your data\n",
    "\n",
    "The first thing to do when opening a new dataset is print out a few rows to keep as a visual reference. We accomplish this with `head`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head() \n",
    "\n",
    "#movies_df.head(10)  # you can specify how many rows you want to see by passing a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the last five rows use `tail`, which also accepts a number, and in this case we printing the bottom two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting info about your data\n",
    "\n",
    "`info` should be one of the very first commands you run after loading your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fast and useful attribute is `shape`, which outputs just a tuple of (rows, columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates\n",
    "\n",
    "This dataset does not have duplicate rows, but it is always important to verify you aren't aggregating duplicate rows.\n",
    "\n",
    "To demonstrate, let's simply just double up our movies DataFrame by appending it to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([movies_df, movies_df])\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try dropping duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dups = temp_df.drop_duplicates()\n",
    "\n",
    "#no_dups.shape\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `append`, the `drop_duplicates()` method will also return a copy of your `DataFrame`, but this time with duplicates removed. Calling `shape` confirms we're back to the 1000 rows of our original dataset.\n",
    "\n",
    "It's a little verbose to keep assigning `DataFrames` to the same variable like in this example. For this reason, pandas has the `inplace` keyword argument on many of its methods. Using `inplace=True` will modify the `DataFrame` object in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.drop_duplicates(inplace=True)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important argument for `drop_duplicates()` is `keep`, which has three possible options:\n",
    "\n",
    "* `first`: (default) Drop duplicates except for the first occurrence.\n",
    "* `last`: Drop duplicates except for the last occurrence.\n",
    "* `False`: Drop all duplicates.\n",
    "\n",
    "Since we didn't define the `keep` argument in the previous example it was defaulted to `first`. This means that if two rows are the same pandas will drop the second row and keep the first row. Using `last` has the opposite effect: the first row is dropped.\n",
    "\n",
    "`keep`, on the other hand, will drop all duplicates. If two rows are the same then both will be dropped. Watch what happens to `temp_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([movies_df, movies_df]) # make a new copy\n",
    "\n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all rows were duplicates, `keep=False` dropped them all resulting in zero rows being left over. If you're wondering why you would want to do this, one reason is that it allows you to locate all duplicates in your dataset. When conditional selections are shown below you'll see how to do that.\n",
    "\n",
    "### Column cleanup\n",
    "\n",
    "Many times datasets will have verbose column names with symbols, upper and lowercase words, spaces, and typos. To make selecting data by column name easier we can spend a little time cleaning up their names.\n",
    "\n",
    "Here's how to print the column names of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `rename` method to rename certain or all columns via a dict. We don't want parentheses, so let's rename those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.rename(columns={\n",
    "        'Runtime (Minutes)': 'Runtime', \n",
    "        'Revenue (Millions)': 'Revenue_millions'\n",
    "    }, inplace=True)\n",
    "\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. But what if we want to lowercase all names? Instead of using `rename` we could also iterate the columns using a comprehension, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.columns = [col.lower() for col in movies_df.columns]\n",
    "\n",
    "#columns = []\n",
    "#for col in movies_df:\n",
    "#     columns.append(col.lower())\n",
    "    \n",
    "#movies_df.columns = columns\n",
    "\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to lowercase, remove special characters, and replace spaces with underscores if you'll be working with a dataset for some time.\n",
    "\n",
    "### How to work with missing values\n",
    "\n",
    "When exploring data, you’ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you'll see Python's `None` or NumPy's `np.nan`, each of which are handled differently in some situations.\n",
    "\n",
    "There are two options in dealing with nulls: \n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values, a technique known as **imputation**\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our `DataFrame` are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice `isnull` returns a `DataFrame` where each cell is either True or False depending on that cell's null status.\n",
    "\n",
    "To count the number of nulls in each column we use an aggregate function for summing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing rows or columns with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientists and Analysts regularly face the dilemma of dropping or imputing null values, and is a decision that requires intimate knowledge of your data and its context. Overall, removing null data is only suggested if you have a small amount of missing data.\n",
    "\n",
    "Remove nulls is pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation will delete any **row** with at least a single null value, but it will return a new DataFrame without altering the original one. You could specify `inplace=True` in this method as well.\n",
    "\n",
    "So in the case of our dataset, this operation would remove 128 rows where `revenue_millions` is null and 64 rows where `metascore` is null. This obviously seems like a waste since there's perfectly good data in the other columns of those dropped rows. That's why we'll look at imputation next.\n",
    "\n",
    "Other than just dropping rows, you can also drop columns with null values by setting `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, this operation would drop the `revenue_millions` and `metascore` columns.\n",
    "\n",
    ">**Side note**: What's with this `axis=1` parameter?\n",
    "\n",
    "It's not immediately obvious where `axis` comes from and why you need it to be 1 for it to affect columns. To see why, just look at the `.shape` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned above, this is a tuple that represents the shape of the DataFrame, i.e. 1000 rows and 11 columns. Note that the *rows* are at index zero of this tuple and *columns* are at **index one** of this tuple. This is why `axis=1` affects columns. This comes from NumPy, and is a great example of why learning NumPy is worth your time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation\n",
    "\n",
    "Imputation is a conventional feature engineering technique used to keep valuable data that have null values. \n",
    "\n",
    "There may be instances where dropping every row with a null value removes too big a chunk from your dataset, so instead we can impute that null with another value, usually the **mean** or the **median** of that column. \n",
    "\n",
    "Let's look at imputing the missing values in the `revenue_millions` column. First we'll extract that column into its own variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = movies_df['revenue_millions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using square brackets is the general way we select columns in a DataFrame. \n",
    "\n",
    "If you remember back to when we created DataFrames from scratch, the keys of the `dict` ended up as column names. Now when we select columns of a DataFrame, we use brackets just like if we were accessing a Python dictionary. \n",
    "\n",
    "`revenue` now contains a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different formatting than a DataFrame, but we still have our `Title` index. \n",
    "\n",
    "We'll impute the missing values of revenue using the mean. Here's the mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_mean = revenue.mean()\n",
    "\n",
    "revenue_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the mean, let's fill the nulls using `fillna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue.fillna(revenue_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now replaced all nulls in `revenue` with the mean of the column. Notice that by using `inplace=True` we have actually affected the original `movies_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing an entire column with the same value like this is a basic example. It would be a better idea to try a more granular imputation by Genre or Director. \n",
    "\n",
    "For example, you would find the mean of the revenue generated in each genre individually and impute the nulls in each genre with that genre's mean.\n",
    "\n",
    "Let's now look at more ways to examine and understand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding your variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `describe` on an entire DataFrame we can get a summary of the distribution of continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding which variables are continuous also comes in handy when thinking about the type of plot to use to represent your data visually. \n",
    "\n",
    "`describe` can also be used on a categorical variable to get the count of rows, unique count of categories, top category, and freq of top category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['genre'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the genre column has 207 unique values, the top value is Action/Adventure/Sci-Fi, which shows up 50 times (freq).\n",
    "\n",
    "`value_counts` can tell us the frequency of all values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['genre'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationships between continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the correlation method `.corr()` we can generate the relationship between each continuous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.corr(numeric_only=True)\n",
    "\n",
    "#import numpy as np\n",
    "#movies_df.select_dtypes(include=np.number).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation tables are a numerical representation of the bivariate relationships in the dataset. \n",
    "\n",
    "Positive numbers indicate a positive correlation — one goes up the other goes up — and negative numbers represent an inverse correlation — one goes up the other goes down. 1.0 indicates a perfect correlation. \n",
    "\n",
    "So looking in the first row, first column we see `rank` has a perfect correlation with itself, which is obvious. On the other hand, the correlation between `votes` and `revenue_millions` is 0.6. A little more interesting.\n",
    "\n",
    "Examining bivariate relationships comes in handy when you have an outcome or dependent variable in mind and would like to see the features most correlated to the increase or decrease of the outcome. You can visually represent bivariate relationships with scatterplots (seen below in the plotting section). \n",
    "\n",
    "For a deeper look into data summarizations check out [Essential Statistics for Data Science](https://www.learndatasci.com/tutorials/data-science-statistics-using-python/).\n",
    "\n",
    "Let's see more about manipulating DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing, selecting, extracting\n",
    "\n",
    "Up until now we've focused on some basic summaries of our data. We've learned about simple column extraction using single brackets, and we imputed null values in a column using `fillna`. Below are the other methods of slicing, selecting, and extracting you'll need to use constantly.\n",
    "\n",
    "It's important to note that, although many methods are the same, DataFrames and Series have different attributes, so you'll need be sure to know which type you are working with or else you will receive attribute errors. \n",
    "\n",
    "Let's look at working with columns first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By column\n",
    "\n",
    "You already saw how to extract a column using square brackets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_col = movies_df['genre']\n",
    "\n",
    "type(genre_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return a `Series`. To extract a column as a `DataFrame`, you need to pass a list of column names. In our case that's just a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_col = movies_df[['genre']]\n",
    "\n",
    "type(genre_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's just a list, adding another column name is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = movies_df[['genre', 'rating']]\n",
    "\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at getting data by rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rows, we have two options: \n",
    "\n",
    "- `loc` - **loc**ates by name\n",
    "- `iloc`- **loc**ates by numerical **i**ndex\n",
    "\n",
    "Remember that we are still indexed by movie Title, so to use `loc` we give it the Title of a movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom = movies_df.loc[\"Prometheus\"]\n",
    "\n",
    "prom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, with `iloc` we give it the numerical index of Prometheus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom = movies_df.iloc[1]\n",
    "\n",
    "prom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc` and `iloc` can be thought of as similar to Python `list` slicing. To show this even further, let's select multiple rows.\n",
    "\n",
    "How would you do it with a list? In Python, just slice with brackets like `example_list[1:4]`. It's works the same way in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_subset = movies_df.loc['Prometheus':'Sing']\n",
    "\n",
    "#movie_subset = movies_df.iloc[1:4]\n",
    "\n",
    "movie_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important distinction between using `loc` and `iloc` to select multiple rows is that `loc` includes the movie *Sing* in the result, but when using `iloc` we're getting rows 1:4 but the movie at index 4 (*Suicide Squad*) is not included. \n",
    "\n",
    "Slicing with `iloc` follows the same rules as slicing with lists, the object at the index at the end is not included.\n",
    "\n",
    "#### Conditional selections\n",
    "We’ve gone over how to select columns and rows, but what if we want to make a conditional selection? \n",
    "\n",
    "For example, what if we want to filter our movies DataFrame to show only films directed by Ridley Scott or films with a rating greater than or equal to 8.0?\n",
    "\n",
    "To do that, we take a column from the DataFrame and apply a Boolean condition to it. Here's an example of a Boolean condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (movies_df['director'] == \"Ridley Scott\")\n",
    "\n",
    "condition.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `isnull`, this returns a `Series` of `True` and `False` values: True for films directed by Ridley Scott and False for ones not directed by him. \n",
    "\n",
    "We want to filter out all movies not directed by Ridley Scott, in other words, we don’t want the False films. To return the rows where that condition is True we have to pass this operation into the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df['director'] == \"Ridley Scott\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get used to looking at these conditionals by reading it like: \n",
    "\n",
    "> Select movies_df where movies_df director equals Ridley Scott\n",
    "\n",
    "Let's look at conditional selections using numerical values by filtering the DataFrame by ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df['rating'] >= 8.6].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some richer conditionals by using logical operators `|` for \"or\" and `&` for \"and\".\n",
    "\n",
    "Let's filter the the DataFrame to show only movies by Christopher Nolan OR Ridley Scott:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[(movies_df['director'] == 'Christopher Nolan') | (movies_df['director'] == 'Ridley Scott')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure to group evaluations with parentheses so Python knows how to evaluate the conditional.\n",
    "\n",
    "Using the `isin` method we could make this more concise though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df['director'].isin(['Christopher Nolan', 'Ridley Scott'])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want all movies that were released between 2005 and 2010, have a rating above 8.0, but made below the 25th percentile in revenue.\n",
    "\n",
    "Here's how we could do all of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\n",
    "    ((movies_df['year'] >= 2005) & (movies_df['year'] <= 2010))\n",
    "    & (movies_df['rating'] > 8.0)\n",
    "    & (movies_df['revenue_millions'] < movies_df['revenue_millions'].quantile(0.25))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall up when we used `describe` the 25th percentile for revenue was about 17.4, and we can access this value directly by using the `quantile` method with a float of 0.25.\n",
    "\n",
    "So here we have only four movies that match that criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using query()\n",
    "\n",
    "Pandas introduced the `query()` method in order to facilitate the data selection methods we've seen above.\n",
    "\n",
    "Let's go through some of the selections we made above, using `query()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.query('director == \"Ridley Scott\"')\n",
    "\n",
    "#movies_df.query('rating >= 8.6')\n",
    "\n",
    "#movies_df.query('director in [\"Ridley Scott\", \"Christopher Nolan\"]')\n",
    "\n",
    "#movies_df.query('(year >= 2005 and year <= 2010) and rating > 8.0 \\\n",
    "#                and revenue_millions < revenue_millions.quantile(0.25)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use external variables inside query():\n",
    "revenue_mean = movies_df.revenue_millions.mean()\n",
    "movies_df.query('revenue_millions > @revenue_mean')\n",
    "\n",
    "# we can also directly compare two columns. Since our dataframe lacks two comparable columns, \n",
    "# here's a ficticious example:\n",
    "# my_df.query('sales > expenses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, you can apply the query directly in the dataframe:\n",
    "movies_df_copy = movies_df.copy()\n",
    "movies_df_copy.query('rating >= 8.6', inplace=True)\n",
    "movies_df_copy.shape\n",
    "\n",
    "#movies_df_copy = movies_df.query('rating >= 8.6').copy()\n",
    "#movies_df_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "Another common need is sorting the data. There are two different ways of sorting: one is sorting by column names and another by the dataframe's index (Title in our example).\n",
    "\n",
    "So if we want to sort by Title, since it's an index we have to do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're sorting by axis 0, i.e. by rows, in descending mode\n",
    "movies_df.sort_index(axis = 0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what happens when we sort by axis 1, i.e. columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.sort_index(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns got sorted and shifted positions.\n",
    "\n",
    "Now if we wanted to sort by a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.sort_values(by='director', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can sort my multiple columns as well\n",
    "movies_df.sort_values(by=['director', 'Title'], ascending=True)\n",
    "\n",
    "# and specify a different sorting order for each column\n",
    "#movies_df.sort_values(by=['director', 'Title'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vlookup / joins\n",
    "\n",
    "It's common to have multiple datasources that you'll need to join, usually using a column that exists on both datasources. It's a operation similar to a `join` in an SQL database or a `vlookup` on Excel.\n",
    "\n",
    "Let's load two datasets and join them on a mutual 'key' column. Note that **the 'key' must be an index in both dataframes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchases_join.csv')\n",
    "df_2 = pd.read_csv('purchases_join_2.csv')\n",
    "\n",
    "df.set_index('name', inplace=True)\n",
    "df_2.set_index('name', inplace=True)\n",
    "joined = df.join(df_2, on='name')\n",
    "\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method, that doesn't need indexes, is the `merge` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df, df_2, on='name')\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you have two datasets with the same columns, and only want to join the rows from both? Use the `concat` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchases_concat.csv')\n",
    "df_2 = pd.read_csv('purchases_concat_2.csv')\n",
    "\n",
    "# to use when datasets have no headers or different headers\n",
    "#df = pd.read_csv('purchases_concat.csv', header=None)\n",
    "#df_2 = pd.read_csv('purchases_concat_2.csv', header=None)\n",
    "\n",
    "concatenated = pd.concat([df, df_2], ignore_index=True)\n",
    "\n",
    "concatenated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `ignore_index=True`: we're saying that we don't want the indexes from the dataframes we're concatenating, but instead have a new index built from the concatenation. Try to remove it to see the difference in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot / groupby\n",
    "\n",
    "It's also common the need to group and aggregate values based on one or more columns of your dataset. It's a operation similar to a `group by` in an SQL database or a `pivot` table in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment and uncomment the examples below to see different group by's\n",
    "df = pd.read_csv('orders.csv')\n",
    "\n",
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count orders belonging to each user\n",
    "df.groupby(['user_id']).agg({'order_id': 'count'})\n",
    "\n",
    "# count orders and sum total value of orders by user\n",
    "#df.groupby(['user_id']).agg({'order_id': 'count', 'total': 'sum'})\n",
    "\n",
    "# same as above, but instead of sum we want the average order value for each user\n",
    "#df.groupby(['user_id']).agg({'order_id': 'count', 'total': 'mean'})\n",
    "\n",
    "# group orders by user_id\n",
    "#df.groupby(['user_id', 'order_id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#exporting the pivot table\n",
    "pivot = df.groupby(['user_id', 'order_id']).sum()\n",
    "\n",
    "pivot.to_csv('pivot.csv')\n",
    "pivot.to_json('pivot.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying functions\n",
    "\n",
    "It is possible to iterate over a `DataFrame` or `Series` as you would with a list, but doing so — especially on large datasets — is very slow.\n",
    "\n",
    "An efficient alternative is to `apply` a function to the dataset. For example, we could use a function to convert movies with an 8.0 or greater to a string value of \"good\" and the rest to \"bad\" and use this transformed values to create a new column.\n",
    "\n",
    "First we would create a function that, when given a rating, determines if it's good or bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_function(x):\n",
    "    if x >= 8.0:\n",
    "        return \"good\"\n",
    "    else:\n",
    "        return \"bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to send the entire rating column through this function, which is what `apply()` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(rating_function)\n",
    "\n",
    "movies_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.apply` method passes every value in the `rating` column through the `rating_function` and then returns a new `Series`. This `Series` is then assigned to a new column called `rating_category`.\n",
    "\n",
    "You can also use anonymous functions as well. This lambda function achieves the same result as `rating_function`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(lambda x: 'good' if x >= 8.0 else 'bad')\n",
    "\n",
    "movies_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, using `apply` will be much faster than iterating manually over rows because pandas is utilizing vectorization.\n",
    "\n",
    "> Vectorization: a style of computer programming where operations are applied to whole arrays instead of individual elements —[Wikipedia](https://en.wikipedia.org/wiki/Vectorization)\n",
    "\n",
    "A good example of high usage of `apply` is during natural language processing (NLP) work. You'll need to apply all sorts of text cleaning functions to strings to prepare for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Plotting\n",
    "\n",
    "Another great thing about pandas is that it integrates with Matplotlib, so you get the ability to plot directly off DataFrames and Series. To get started we need to import Matplotlib (`pip install matplotlib`), unless you're using Ananconda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 20, 'figure.figsize': (10, 8)}) # set font and plot size to be larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin. There won't be a lot of coverage on plotting, but it should be enough to explore you're data easily.\n",
    "\n",
    "**Side note:**\n",
    "For categorical variables use Bar Charts and Boxplots.  For continuous variables utilize Histograms, Scatterplots, Line graphs, and Boxplots.\n",
    "\n",
    "Let's plot the relationship between ratings and revenue. All we need to do is call `plot` on `movies_df` with some info about how to construct the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.plot(kind='scatter', x='rating', y='revenue_millions', title='Revenue (millions) vs Rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's with the semicolon? It's not a syntax error, just a way to hide the `<matplotlib.axes._subplots.AxesSubplot at 0x26613b5cc18>` output when plotting in Jupyter notebooks.\n",
    "\n",
    "If we want to plot a simple Histogram based on a single column, we can call plot on a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['rating'].plot(kind='hist', title='Rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the `describe` example at the beginning of this tutorial? Well, there's a graphical representation of the interquartile range, called the Boxplot. Let's recall what `describe` gives us on the ratings column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Boxplot we can visualize this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['rating'].plot(kind=\"box\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"https://i1.wp.com/flowingdata.com/wp-content/uploads/2008/02/box-plot-explained.gif\" />\n",
    "    <figcaption>Source: <em>Flowing Data</em></figcaption>\n",
    "</figure>\n",
    "<br>\n",
    "\n",
    "By combining categorical and continuous data, we can create a Boxplot of revenue that is grouped by the Rating Category we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.boxplot(column='revenue_millions', by='rating_category');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the general idea of plotting with pandas. There's too many plots to mention, so definitely take a look at the `plot` [docs here](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) for more information on what it can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "Exploring, cleaning, transforming, and visualization data with pandas in Python is an essential skill in data science. Just cleaning wrangling data is 80% of your job as a Data Scientist. After a few projects and some practice, you should be very comfortable with most of the basics.\n",
    "\n",
    "To keep improving, view the [extensive tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html) offered by the official pandas docs.\n",
    "\n",
    "If you want to understand how pandas works step by step, take a look at the awesome [Pandas Tutor](https://pandastutor.com/vis.html) tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
